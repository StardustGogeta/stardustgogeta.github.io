<!DOCTYPE html>
<head>
    <title>CMSC250 Handbook</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous"></script>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet"
		integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous" defer></script>
    <script src="../math.js" defer></script>
</head>
<br>
<div class='container'>
<h1>Probability and Combinatorics</h1>
If you've ever played a game of Texas Hold'em poker, you're familiar with the fact that certain hands are worth more than others. 
For example, <a href="https://www.cardplayer.com/rules-of-poker/hand-rankings">a flush is better than a straight, and a full house is better than either</a>. Why is this the case?<br>
You may have some intuitive understanding that the more rare a hand is, the higher its rank. Using the ideas discussed here, we will be able to prove this relationship holds.
<hr>

<h2>Addition and Multiplication Principles</h2>
Combinatorics is all about <i>counting</i>. In particular, we often count the number of ways in which we can perform an action or a set of actions.
The <a href="https://brilliant.org/wiki/rule-of-sum-and-rule-of-product-problem-solving/">addition and multiplication principles</a> help with this analysis.
<ul>
    <li>The <b>addition principle</b> (or "rule of sum") states that given two different actions where there are `x` choices for the first one
        and `y` choices for the second, there are `x+y` ways of doing either, assuming it is not possible to do both actions simultaneously.</li>
    <li>The <b>multiplication principle</b> (or "rule of product") states that given two different actions where there are `x` choices for the first one
        and `y` choices for the second, there are `xy` ways of performing both actions together (in any order).</li>
</ul>

<card class="card my-3">
    <h5 class="card-header">Example</h5>
    <div class="card-body">
        Consider a person ordering a single meal at an Italian restaurant. The menu has only two items: specialty pizza and salad.
        The salad consists only of lettuce and dressing, where there are three choices of lettuce and two types of dressing.
        Meanwhile, the pizza comes in one of four different pre-made varieties. How many total meal options are there?
        <hr>
        We have two choices, pizza or salad, so we count the number of possible salads and the number of possible pizzas and add them together by the addition principle.
        The pizza clearly has only four options, as was given. However, counting the number of salad choices we have requires the multiplication rule.
        With `3` choices of lettuce and `2` choices of dressing, we have `2\times3=6` ways of picking one of each to form a salad.
        Thus, we obtain a final answer of `4+6=10` total possible meal selections.
    </div>
</card>

<h2>Combinations and Permutations</h2>
Two fundamental concepts in the field of combinatorics are those of <b>combinations</b> and <b>permutations</b>.<br>
A permutation is an ordering of some collection of elements.
<!--<sup><a tabindex="0" role="button" data-bs-toggle="popover" data-bs-trigger="hover" data-bs-content="Some sources make the distinction that the permutation is not the collection itself but simply the ordering. This makes no practical difference.">[1]</a></sup>-->
The number of possible permutations of a set of `n` distinct elements is simply given by `n!`, where `!` denotes the factorial function.
<card class="card my-3">
    <h5 class="card-header">Derivation</h5>
    <div class="card-body">
        Suppose we have a set of `n` distinct elements. We want to create one permutation of this set.<br>
        We begin by picking the first element of the permutation. There are `n` elements to choose from, so there are `n` ways of doing this.
        Then, we select the second element of the permutation. There are now only `n-1` elements remaining, so there are `n-1` ways of doing this.
        We repeat until we have exhausted all elements of the original set, leaving only one choice for the last element in the permutation.
        By the multiplication principle, there are `n\cdot(n-1)\cdots1=n!` ways of performing this entire task.<br>
        Every unique sequence of choices results in a new permutation (and every permutation can be constructed in this manner),
        so there are exactly `n!` possible permutations of the original set.
    </div>
</card>

An ordering of `k` elements from a set of `n` elements is called a `k`-permutation. The number of possible `k`-permutations from a set of size `n` is given by
$$P(n, k)={}_nP_k=\frac{n!}{(n-k)!}.$$

<card class="card my-3">
    <h5 class="card-header">Derivation</h5>
    <div class="card-body">
        Suppose we have a set of `n` distinct elements. We want to create a permutation of size `k` from this set.<br>
        We begin by picking the first element of the permutation. There are `n` elements to choose from, so there are `n` ways of doing this.
        Then, we select the second element of the permutation. There are now only `n-1` elements remaining, so there are `n-1` ways of doing this.
        We repeat for a total of `k` elements, leaving `n-k` elements unselected.
        By the multiplication principle, there are `n\cdot(n-1)\cdots(n-k+1)=n!/(n-k)!` ways of performing this entire task.<br><br>
        Alternatively, we can first notice that there are `n!` permutations of all elements in the original set. By extracting the first `k` elements of
        any given permutation, we construct a `k`-permutation as desired. However, there are `n-k` elements not selected, and the `k`-permutation is repeated
        once for each of their `(n-k)!` possible arrangements. Dividing out the number of repetitions yields `n!/(n-k)!` as before.
    </div>
</card>

A combination is simply a set of elements without respect to order. For instance, the set `\{1, 5, 7\}` is a combination of three odd numbers.
This is <i>exactly the same</i> as the combinations `\{5, 7, 1\}`, `\{1, 7, 5\}`, etc., since order does not matter.<br>

The number of possible combinations of `k` elements from a set of size `n` is given by
$$C(n, k)={}_nC_k=\binom{n}{k}=\frac{n!}{k!(n-k)!}.$$

<card class="card my-3">
    <h5 class="card-header">Derivation</h5>
    <div class="card-body">
        Suppose we have a set of `n` distinct elements. We want to create a combination of size `k` from this set.<br>
        Note that from any combination of `k` elements, we can rearrange to form `k!` different `k`-permutations which correspond to it.
        Since we know that there are `P(n,k)=n!/(n-k)!` total `k`-permutations, there must be `P(n,k)/k!` total `k`-combinations, or
        $$\frac{n!}{k!(n-k)!}.$$
    </div>
</card>

<!-- Pascal's triangle, common combinatorial identities like choose 1, choose 0, Pascal identity -->

<h2>Preliminary Probability Definitions</h2>
There are a few ideas we need to define immediately. A lot of this may be familiar or intuitive, but it is still important to discuss.
A good source of information can be found <a href="https://www.math.tamu.edu/~Janice.Epstein/141/review/WIR10_Prob_1.pdf">here</a>, summarized below.
Other resources include <a href="https://www.statisticshowto.com/complementary-events/">Statistics How To</a> and <a href="https://mathworld.wolfram.com/Experiment.html">Wolfram MathWorld</a>,
though the latter can be fairly technical.
<ul>
    <li>An <b>experiment</b> is an activity with an observable result. (ex. Rolling a die)</li>
    <li>An <b>outcome</b> is a possible result of an experiment. (ex. Rolling a 5)</li>
    <li>The <b>sample space</b> is the set of all possible outcomes of an experiment. (Rolling a 1, 2, 3, 4, 5, or 6)</li>
    <li>An <b>event</b> is a subset of the sample space. (Rolling a 1, 3, or 5)</li>
    <ul>
        <li>An event can include some, none, or all of the events in the sample space.</li>
    </ul>
    <li>The <b>probability</b> of an event is the chance that it occurs.</li>
    <ul>
        <li>If all outcomes in a sample space have the same probability, then the probability of an event `E` occurring is simply
            the ratio of the number of outcomes in `E` to the total number of outcomes in the sample space.</li>
    </ul>
    <li>Two events are <b>disjoint / mutually exclusive</b> if they share no outcomes.</li>
    <ul>
        <li>Alternatively, we might say that the intersection of the two events is empty.</li>
    </ul>
    <li>Two events are <b>complementary</b> if they are disjoint and their union is the sample space (so exactly one must occur).</li>
    <ul>
        <li>If the first event is `A`, then the other is the <b>complement</b> of `A`, often denoted `A^c`.</li>
    </ul>
    <li>Two events `A` and `B` are <b>independent</b> if the probability of `A` does not depend on whether `B` occurs, and vice versa.</li>
    <ul>
        <li>If two events are not independent, then they are said to be <b>dependent</b>.</li>
    </ul>
</ul>
Several of these concepts involve basic set theory like union and intersection, so be sure to understand the meanings of those terms first.

<!-- Link to union / intersection, or discuss here via Venn diagram -->

<br><br>

<h2>Probability Axioms</h2>
There are <a href="https://www.probabilitycourse.com/chapter1/1_3_2_probability.php">three axioms</a> that form the foundation of probability theory, listed below.
<ol>
    <li>For any event `E`, we have `P(E)\ge0`.</li>
    <li>For a sample space `S`, we have `P(S)=1`.</li>
    <li>For <i>disjoint</i> events `E_1` through `E_n`, we have `P(E_1\cup E_2\cup\cdots\cup E_n)=P(E_1)+P(E_2)+\cdots+P(E_n)`.</li>
</ol>
These facts should be fairly intuitive. For the first one, we know that nothing should ever have a negative chance of occurring.
For the second, it is clear by definition that if we take the probability that an experiment's outcome is in the entire sample space, we will find this chance to be 100%.
The third should also be fairly obvious. For example, when rolling a fair die, if the chance of rolling a 1 is `1/6` and the chance of rolling a 2 or 3 is `1/3`,
then the chance of rolling a 1, 2, or 3 is `1/6+1/3=1/2`.
<br><br>
There are many important results that can be derived from these axioms, some of which are given below.
Proofs are available on <a href="https://en.wikipedia.org/wiki/Probability_axioms#Consequences">the associated Wikipedia page</a>.
<ul>
    <li>For events `A` and `B`, if `A\subseteq B`, then `P(A)\le P(B)`.</li>
    <li>The probability of the empty event `\emptyset` is `P(\emptyset)=0`.</li>
    <ul>
        <li>Note that it is still possible for a nonempty event to have zero probability, as well.</li>
    </ul>
    <li>For any event `A`, the probability of its complement is `P(A^c)=1-P(A)`.</li>
    <li>For any event `A`, its probability must satisfy `0\le P(A)\le1`.</li>
</ul>

One of the most important results is known as the <b>inclusion-exclusion principle</b>, which generalizes the third axiom to non-disjoint events.
Namely, we see that $$P(A\cup B)=P(A)+P(B)-P(A\cap B)$$ for any two events `A` and `B`. In essence, we remove the intersection because it is erroneously double-counted otherwise.
When we extend to the union of three or more events, the formulas become far more complicated.
$$\begin{align*}P(A\cup B\cup C)&=P(A)+P(B)+P(C)\\&-P(A\cap B)-P(B\cap C)-P(A\cap C)\\&+P(A\cap B\cap C)\end{align*}$$
$$\begin{align*}P(A\cup B\cup C\cup D)&=P(A)+P(B)+P(C)+P(D)\\
&-P(A\cap B)-P(A\cap C)-P(A\cap D)-P(B\cap C)-P(B\cap D)-P(C\cap D)\\
&+P(A\cap B\cap C)+P(A\cap B\cap D)+P(A\cap C\cap D)+P(B\cap C\cap D)\\
&-P(A\cap B\cap C\cap D)
\end{align*}$$

We proceed as before, adding the individual probabilities and then removing the double-counted pairwise intersections. However, this process removes
the three-way intersections too many times, so we need to add those back. If there are more than three events, then this will result in over-counting of
the four-way intersections, and we must remove one copy of each of those.<br><br>

Fortunately for us, <a href="https://proofwiki.org/wiki/Inclusion-Exclusion_Principle">it can be proven</a> that this pattern holds in general.
When applying the formula, simply be careful to include all required terms and to have the correct sign on each.
The number of terms can be easily checked - the number of `x`-way intersection terms from a set of `y` events is simply given by `\binom yx`.
Additionally, we see that the total number of terms for a set of `y` events should be `\sum_{i=1}^y\binom yi=2^y-1`.<br><br>

It should be noted that the inclusion-exclusion principle applies equivalently to set cardinality in place of probability since both are
<a href="https://proofwiki.org/wiki/Definition:Additive_Function_(Measure_Theory)">additive functions</a>
(i.e., the probabilities and cardinalities of disjoint unions are the sums of the components).

<br><br>

<h2>Conditional Probability and Bayes' Theorem</h2>
A common question to ask is "what is the chance of event `B` occurring given that event `A` occurs?" Questions like these fall into the realm of <b>conditional probability</b>.
We denote the probability of event `B` given event `A` as `P(B\mid A)`, with a vertical bar in-between. This quantity is defined as follows:
$$P(B\mid A)\overset{\text{def}}{=}\frac{P(A\cap B)}{P(A)}.$$

For example, consider rolling a fair six-sided die. Let event `A` be rolling an odd number, and let `B` be rolling a number less than 4.
It is trivial to find that `P(A)=1/2` and `P(A\cap B)=1/3`, and so we find `P(B\mid A)=(1/3)/(1/2)=2/3` as the chance of rolling less than 4 given that an odd number is rolled.
This makes senseâ€”it is either a 1, 3, or 5 with uniform probability, and exactly two of these three options are less than 4.<br><br>

<b>Bayes' theorem</b> (or "law," or "rule") takes this definition and expands it a bit. Since `P(A\cap B)=P(B\cap A)` (by commutativity of set intersection), we can write
$$P(A\cap B)=P(A)\cdot P(B\mid A)=P(B)\cdot P(A\mid B).$$

Then, we can simply rearrange to obtain that
$$P(A\mid B)=P(B\mid A)\left(\frac{P(A)}{P(B)}\right).$$

Finally since `P(B)=P(B\cap(A\cup A^c))=P((A\cap B) \cup (A^c\cap B))=P(A\cap B)+P(A^c\cap B)=P(A)\cdot P(B\mid A)+P(A^c)\cdot P(B\mid A^c)`, we can rewrite the fraction as
$$P(A\mid B)=\frac{P(A)\cdot P(B\mid A)}{P(A)\cdot P(B\mid A)+P(A^c)\cdot P(B\mid A^c)}.$$

Note how the numerator term appears in the denominator as well. Note also that `A` and `A^c` are not particularly special; we simply need a set of mutually exclusive
and collectively exhaustive events, and each one will contribute a separate term to the denominator.

<card class="card my-3">
    <h5 class="card-header">Example</h5>
    <div class="card-body">
        Consider an engineer ordering parts for his or her latest project. The engineer orders some gizmos from each of three companies: Advanced Applications (A), Dynamic Dynamos (D), and Mechanical Machines (M).
        One half of all gizmos are ordered from A, one third are from D, and the remaining gizmos are ordered from M.
        Unfortunately, none of these companies have perfect assembly lines, and each will occasionally produce defective parts.
        A randomly selected gizmo produced by A has a `1/100` chance of being defective, whereas the chances of gizmos by D or M being defective are `1/200` and `1/50`, respectively.
        If a gizmo is randomly selected from the set of all gizmos ordered and it is found to be defective, what are the odds it was made by Dynamic Dynamos?
        <hr>
        From the problem statement, we know that `P(A)=1/2`, `P(D)=1/3`, and `P(M)=1/6`.
        Additionally, `P(\text{Defect}\mid A)=1/100`, `P(\text{Defect}\mid D)=1/200`, and `P(\text{Defect}\mid M)=1/50`.
        Applying Bayes' theorem yields
        $$\begin{align*}
        P(D\mid\text{Defect})&=\frac{P(\text{Defect}\cap D)}{P(\text{Defect})}\\
        &=\frac{P(\text{Defect}\cap D)}{P(\text{Defect}\cap A)+P(\text{Defect}\cap D)+P(\text{Defect}\cap M)}\\
        &=\frac{P(\text{Defect}\mid D)\cdot P(D)}{P(\text{Defect}\mid A)\cdot P(A) + P(\text{Defect}\mid D)\cdot P(D) + P(\text{Defect}\mid M)\cdot P(M)}\\
        &=\frac{1/200\cdot 1/3}{1/100\cdot 1/2 + 1/200\cdot 1/3 + 1/50\cdot 1/6}\\
        &=\frac{1/600}{3/600 + 1/600 + 2/600}\\
        P(D\mid\text{Defect})&=\frac16.
        \end{align*}$$
        Thus, we find that the probability that a defective gizmo selected at random originated from Dynamic Dynamos is `1/6`.
    </div>
</card>

A more visual way of representing conditional probability is with <b>probability trees</b>.
A <a href="https://www.mathsisfun.com/data/probability-tree-diagrams.html">probability tree</a> is a tree diagram read from left to right where
nodes represent experiments and branches represent disjoint outcomes/events that might occur, each labeled with their respective probabilities.
In order to find the probability of a certain leaf, simply multiply the conditional probabilities along the edges leading from the root to that leaf.
Then, to obtain the probability of some set of leaves, find their individual probabilities and add them together.
(To verify that a tree diagram has been correctly constructed, the sum of all leaf probabilities should always equal one.)
<br><br>
Tree diagrams can be very helpful in certain situations involving dependent events and conditional probability, but their usefulness is limited
by how rapidly they grow in size. The number of leaves in such a tree grows exponentially with height, and this can very quickly become unwieldy.
<br><br>

<h2>Independence of Events</h2>
Colloquially, two events are independent if they do not impact one another's odds.<br>
Formally, we say that events `A` and `B` are independent if and only if the following condition holds true:
$$P(A\cap B)=P(A)\cdot P(B).$$

If `A` and `B` both have nonzero probability, we can use the definition of conditional probability to obtain these two conditions, each equivalent to the one above:
$$P(A\mid B)=P(A),$$
$$P(B\mid A)=P(B).$$

Notice that if an event does indeed have zero probability, then it is automatically independent of all other events.<br>
Furthermore, if two events of nonzero probability are disjoint, then `P(A\cap B)=0` and they cannot be independent.<br>
Finally, it is possible to find a couple more equivalent conditions (under the assumption `0 < P(A), P(B) < 1`), though these are far less common:
$$P(A\mid B)=P(A\mid B^c),$$
$$P(B\mid A)=P(B\mid A^c).$$

This result is not hard to prove and aligns with our intuition about the notion of event independence.
<br><br>

<h2>Application: Card Games</h2>
One of the best examples of probability in action is playing card games. Take, for example, the game of Texas Hold'em poker.
If you're not familiar with the game, you can read the rules <a href="https://bicyclecards.com/how-to-play/texas-holdem-poker/">here</a>.
In poker, <i>hands</i> consist of five cards each, taken from a standard deck of fifty-two. Each card has a <i>suit</i> (hearts, diamonds, spades, clubs)
and a <i>rank</i> (in ascending order, 2, 3, 4, 5, 6, 7, 8, 9, 10, jack, queen, king, ace). Depending on the exact suit and rank composition of the cards
in one's hand, it will be considered better or worse than (or in rare cases, equal to) another hand. For more information on hand evaluation and ranking,
see <a href="https://www.cardplayer.com/rules-of-poker/hand-rankings">here</a> and <a href="https://automaticpoker.com/poker-basics/what-happens-if-you-have-the-same-hand-in-poker/">here</a>.
<br><br>
Why are certain hands worth more than others? It is because these hands are provably less common!
We can show this using the probability techniques developed above.
Here's the ordered list of poker hands:
<ol>
    <li>Royal Flush</li>
    <li>Straight Flush</li>
    <li>Four of a Kind</li>
    <li>Full House</li>
    <li>Flush</li>
    <li>Straight</li>
    <li>Three of a Kind</li>
    <li>Two Pair</li>
    <li>Pair</li>
    <li>High Card</li>
</ol>
Let's derive the probabilities of each of these hands occurring when five cards are dealt at random from a standard deck.
<br><br>
A <i>royal flush</i> occurs when we have the ten, jack, queen, king, and ace of one suit. The ranks are prescribed by definition, but we have four choices for the
suit, so there are four unique royal flushes that are possible. Since there are `\binom{52}{5}` possible hands in total, each with the same chance of occurring,
we obtain a probability of `4/\binom{52}{5}\approx0.000154\%` for dealing a royal flush.
<br><br>
A <i>straight flush</i> occurs when we have five cards in a row of the same suit (and we do not have a royal flush).
Notice that each straight flush is uniquely identified by its lowest card.
(There is only one possibility for a straight flush that starts with the Five of Hearts, for example.) Now, how many possibilities are there for the lowest card?
There have to be four cards after it in sequence, meaning that 9-10-J-Q-K is the highest we can go (since we do not count royal flushes),
and A-2-3-4-5 is the lowest we can go. Since the lowest card ranges from an ace to a nine in any of the four suits, we have exactly 36 possibilities, and as a result, we know there are 36 unique straight flushes.
We proceed to obtain a probability of `36/\binom{52}{5}\approx0.00139\%` for being dealt a straight flush.

<!-- Application: card games -->
<!-- Application: disease testing -->

<br><br>
